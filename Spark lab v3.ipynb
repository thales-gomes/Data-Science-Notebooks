{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<center>\n    <img src=\"https://gitlab.com/ibm/skills-network/courses/placeholder101/-/raw/master/labs/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n</center>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# **Getting Started with Apache Spark on IBM Cloud**\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Estimated time needed: **15** minutes\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "![](http://spark.apache.org/images/spark-logo.png)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Objectives\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this lab, we will cover some data partioning strategies and methods in Apache Spark and PySpark. We will start with creating the SparkContext and SparkSession. We then create an RDD and apply data partioning. Finally we demonstrate data partioning with dataframes and SparkSQL.\n\nAfter this lab you will be able to:\n\n*   Create the SparkContext and SparkSession\n*   Create an RDD and apply data partioning to it\n*   Create a dataframe and apply data partioning to it\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "***\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Setup\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "For this lab, we are going to be using Python and Spark (PySpark). These libraries should be installed in your lab environment or in SN Labs.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exercise 1 -  Spark Context and Spark Session\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this exercise, you will create the Spark Context and initialize the Spark session needed for SparkSQL and DataFrames.\nSparkContext is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`. SparkSession is needed for SparkSQL and DataFrame operations.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Task 1: Creating the spark session and context\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://jkg-deployment-5cbb3952-d467-45e6-bf30-2052fea47df1-54fc7btns9k:7077) created by getOrCreate at /opt/ibm/kernelScript/python/shell.py:69 ",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-4-9f31e791e3ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating a spark context class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Creating a spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python Spark IBM Cloud Example\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.some.config.option\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"some-value\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda/lib/python/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
                        "\u001b[0;32m/opt/ibm/conda/miniconda/lib/python/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    341\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 343\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    344\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://jkg-deployment-5cbb3952-d467-45e6-bf30-2052fea47df1-54fc7btns9k:7077) created by getOrCreate at /opt/ibm/kernelScript/python/shell.py:69 "
                    ]
                }
            ],
            "source": "# Creating a spark context class\nsc = SparkContext()\n\n# Creating a spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Python Spark IBM Cloud Example\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Task 2: Initialize Spark session\n\nTo work with dataframes we just need to verify that the spark session instance has been created.\nFeel free to click on the \"Spark UI\" button to explore the Spark UI elements.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://172.30.43.34:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.0.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://jkg-deployment-5cbb3952-d467-45e6-bf30-2052fea47df1-54fc7btns9k:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
                        "text/plain": "<pyspark.sql.session.SparkSession at 0x7f87cbb96250>"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "spark"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exercise 2: RDDs\n\nIn this exercise we work with Resilient Distributed Datasets (RDDs). RDDs are Spark's primitive data abstraction and we use concepts from functional programming to create and manipulate RDDs.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Task 1: Create an RDD.\n\nFor demonstration purposes, we create an RDD here by calling `sc.parallelize()`\\\nWe create an RDD which has integers from 1 to 10.\n\nWe then get the number of partitions using the `getNumPartitions()` function and the partitions using the `glom()` function.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Number of partitions: 2\nPartitioner: None\nPartitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
                }
            ],
            "source": "nums = [i for i in range(10)]\n\nrdd = sc.parallelize(nums)\n    \nprint(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd.partitioner))\nprint(\"Partitions structure: {}\".format(rdd.glom().collect()))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In the above cell we can see the default partitions done for the RDD. However, we can change that by passing in an optional second argument to the `parallelize` function.\nLet us now try with 2 and 15 partitions and see how they look like in memory.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Default parallelism: 2\nNumber of partitions: 2\nPartitioner: None\nPartitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
                }
            ],
            "source": "rdd = sc.parallelize(nums, 2)\n    \nprint(\"Default parallelism: {}\".format(sc.defaultParallelism))\nprint(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd.partitioner))\nprint(\"Partitions structure: {}\".format(rdd.glom().collect()))"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "10"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.count()"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Number of partitions: 15\nPartitioner: None\nPartitions structure: [[], [0], [1], [], [2], [3], [], [4], [5], [], [6], [7], [], [8], [9]]\n"
                }
            ],
            "source": "rdd = sc.parallelize(nums, 15)\n\nprint(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd.partitioner))\nprint(\"Partitions structure: {}\".format(rdd.glom().collect()))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Anoter way to partition data is by using the `partitionBy()` function. In this case, the dataset needs to be a tuple with a key/value pair as the default partioner uses a hash for the key to assign elements to a parition.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Number of partitions: 2\nPartitioner: <pyspark.rdd.Partitioner object at 0x7f87cbadcad0>\nPartitions structure: [[(6, 6), (8, 8), (0, 0), (2, 2), (4, 4)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]\npartition: 1 [(6, 6), (8, 8), (0, 0), (2, 2), (4, 4)]\npartition: 2 [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]\n"
                }
            ],
            "source": "rdd = sc.parallelize(nums) \\\n        .map(lambda el: (el, el)) \\\n        .partitionBy(2) \\\n        .persist()\n    \nprint(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd.partitioner))\nprint(\"Partitions structure: {}\".format(rdd.glom().collect()))\n\nj=0\nfor i in rdd.glom().collect():\n    j+=1\n    print(\"partition: \" + str(j) + \" \"+ str(i))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "You can see that now the elements are distributed differently. A few interesting things happened:\n\n```\nparallelize(nums) - we are transforming Python array into RDD with no partitioning scheme,\nmap(lambda el: (el, el)) - transforming data into the form of a tuple,\npartitionBy(2) - splitting data into 2 chunks using default hash partitioner\n```\n\nExplicit assignment of partition locations makes the hashing strategy more apparent. The use of the % function assigns it to the correct partition.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let us now create a more practical dataset of transactions. We have 8 transactions from 4 different geographies as shown below.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "transactions = [\n    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n    {'name': 'Thomas', 'amount': 30, 'country': 'Germany'},\n    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n    {'name': 'Pierre', 'amount': 120, 'country': 'France'},\n    {'name': 'Frank', 'amount': 180, 'country': 'France'}\n]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We know that further analysis will be performed analyzing many similar records within the same country. To optimize network traffic it seems to be a good idea to put records from one country in one node. To meet this requirement, we will need a custom partitioner: Custom partitioner \u2014 function returning an integer for given object (tuple key).\n"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "5901469\n1535451\n6933473\n2199756\n"
                }
            ],
            "source": "# Dummy implementation assuring that data for each country is in one partition\ndef country_partitioner(country):\n    return hash(country)% (10**7+1)\n    #return portable_hash(country)\n    \n\n# Validate results\nprint(country_partitioner(\"Poland\"))\nprint(country_partitioner(\"Germany\"))\nprint(country_partitioner(\"United Kingdom\"))\nprint(country_partitioner(\"France\"))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can see that our custom partitioner creates a unique hash for each country name so it can be used to `partitionBy` our dataset.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Number of partitions: 5\nPartitioner: <pyspark.rdd.Partitioner object at 0x7f87cbb969d0>\nPartitions structure: [[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})], [('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'}), ('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'})], [('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})], []]\n\n--\n\n\npartition: 1\n[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})]\n\npartition: 2\n[('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})]\n\npartition: 3\n[('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'}), ('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'})]\n\npartition: 4\n[('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})]\n\npartition: 5\n[]\n"
                }
            ],
            "source": "rdd = sc.parallelize(transactions) \\\n        .map(lambda el: (el['country'], el)) \\\n        .partitionBy(5, country_partitioner)\n    \nprint(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd.partitioner))\nprint(\"Partitions structure: {}\".format(rdd.glom().collect()))\n\nprint(\"\\n--\\n\")\nfor i, j in enumerate(rdd.glom().collect()):\n    print(\"\\npartition: \" + str(i+1) + \"\\n\"+ str(j))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Using the partitioning scheme, we can now carry out calculations such as total revenue/sales as shown below.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "def sum_sales(iterator):\n    yield sum(transaction[1]['amount'] for transaction in iterator)"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Partitions structure: [[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})], [('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'}), ('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'})], [('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})], []]\nTotal sales for each partition: [115, 300, 230, 126, 0]\n"
                }
            ],
            "source": "by_country = sc.parallelize(transactions) \\\n        .map(lambda el: (el['country'], el)) \\\n        .partitionBy(5, country_partitioner)\n    \nprint(\"Partitions structure: {}\".format(by_country.glom().collect()))\n\n# Sum sales in each partition\nsum_amounts = by_country \\\n    .mapPartitions(sum_sales) \\\n    .collect()\n\nprint(\"Total sales for each partition: {}\".format(sum_amounts))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Exercise 2: DataFrames\n\nIn this exercise we work with DataFrames.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Task 1: Create the DataFrame\n\nWe will now create a DataFrame from the previous \"transactions\" list we created.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/opt/ibm/conda/miniconda/lib/python/site-packages/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
                }
            ],
            "source": "df = spark.createDataFrame(transactions)"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------+--------------+--------+\n|amount|       country|    name|\n+------+--------------+--------+\n|   100|United Kingdom|     Bob|\n|    15|United Kingdom|   James|\n|    51|        Poland|   Marek|\n|   200|       Germany|Johannes|\n|    30|       Germany|  Thomas|\n|    75|        Poland|    Paul|\n|   120|        France|  Pierre|\n|   180|        France|   Frank|\n+------+--------------+--------+\n\n"
                }
            ],
            "source": "df.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Number of partitions: 2\nPartitioner: <pyspark.rdd.Partitioner object at 0x7f87cbb969d0>\nPartitions structure: [[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James'), Row(amount=51, country='Poland', name='Marek'), Row(amount=200, country='Germany', name='Johannes')], [Row(amount=30, country='Germany', name='Thomas'), Row(amount=75, country='Poland', name='Paul'), Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]]\n"
                }
            ],
            "source": "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd.partitioner))\nprint(\"Partitions structure: {}\".format(df.rdd.glom().collect()))"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "partition: 1\n[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James'), Row(amount=51, country='Poland', name='Marek'), Row(amount=200, country='Germany', name='Johannes')]\npartition: 2\n[Row(amount=30, country='Germany', name='Thomas'), Row(amount=75, country='Poland', name='Paul'), Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\n"
                }
            ],
            "source": "for i, j in enumerate(df.rdd.glom().collect()):\n    print(\"partition: \" + str(i+1) + \"\\n\"+ str(j))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "For dataframes we can repartition the dataset by column using the `repartition()` function. The cell below shows how we can partition the dataset by country.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nAfter 'repartition()'\nNumber of partitions: 10\nPartitioner: None\nPartitions structure: [[Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')], [], [Row(amount=200, country='Germany', name='Johannes'), Row(amount=30, country='Germany', name='Thomas')], [], [Row(amount=51, country='Poland', name='Marek'), Row(amount=75, country='Poland', name='Paul')], [Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')], [], [], [], []]\n"
                }
            ],
            "source": "# Repartition by column\ndf2 = df.repartition(10,\"country\")\n\nprint(\"\\nAfter 'repartition()'\")\nprint(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(df2.rdd.partitioner))\nprint(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "partition: 1\n[Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\npartition: 2\n[]\npartition: 3\n[Row(amount=200, country='Germany', name='Johannes'), Row(amount=30, country='Germany', name='Thomas')]\npartition: 4\n[]\npartition: 5\n[Row(amount=51, country='Poland', name='Marek'), Row(amount=75, country='Poland', name='Paul')]\npartition: 6\n[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')]\npartition: 7\n[]\npartition: 8\n[]\npartition: 9\n[]\npartition: 10\n[]\n"
                }
            ],
            "source": "for i, j in enumerate(df2.rdd.glom().collect()):\n    print(\"partition: \" + str(i+1) + \"\\n\"+ str(j))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "These are some of the partitioning strategies for dataframes and RDDs. As the data size increases, the paritioning strategies become more important and can yield significant performance benefits.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "***\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Authors\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "[Karthik Muthuraman](https://www.linkedin.com/in/karthik-muthuraman/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0225ENSkillsNetwork25716109-2021-01-01)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Other Contributors\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "[Jerome Nilmeier](https://github.com/nilmeier)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Change Log\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n| ----------------- | ------- | ---------- | ------------------ |\n| 2021-07-02        | 0.2     | Karthik    | Beta launch        |\n| 2021-06-30        | 0.1     | Karthik    | First Draft        |\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Copyright \u00a9 2021 IBM Corporation. All rights reserved.\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7 with Spark",
            "language": "python3",
            "name": "python37"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}